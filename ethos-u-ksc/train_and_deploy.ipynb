{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Copyright (c) 2021 Arm Limited. All rights reserved.\n",
    "#  SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbfsgUcBG_hx"
   },
   "source": [
    "# Train and Deploy your NPU-enabled models\n",
    "\n",
    "> Using the Arm Corstone-300 with Cortex-M55 and Ethos-U55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jty-u4JWHViE"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook presents a flow to help bridge the gap between data scientists and embedded engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOvd37uuHTE7"
   },
   "source": [
    "## Training a Model\n",
    "\n",
    "In this example we are going to train a \"toy\" model. We will create a basic convolutional neural network model to solve the MNIST problem.\n",
    "\n",
    "The [MNIST database](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits which can be used to train a digit classifier. It is often used as a starter dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EatvozlIIlO"
   },
   "source": [
    "Let's start of by importing the required Python dependencies. For this we will use the [TensorFlow](https://github.com/tensorflow/tensorflow) framework for the model and [TensorFlow Datasets](https://github.com/tensorflow/datasets) to download the MNIST dataset. If you're using Google Colab, these dependencies come preinstalled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3CjmqOzKBY5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJFi-du1IGLt"
   },
   "source": [
    "We can now download the MNIST dataset using TensorFlow datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222,
     "referenced_widgets": [
      "20200e22b37c40f38d0cae9d63828d61",
      "d0332e909a1e41d4b79c78d872426973",
      "a866c812aefb42709c286b504c4b502a",
      "cc2f6b02e2fa4e6b9ba5cadb7c940e38",
      "bedb6318f10e47768eed2b0a76f80169",
      "07a92d5a2ba94fbfa98f8e69e858d1b7",
      "2e5a1d3859f74f7ebf8e3a0436276e4f",
      "b3d302a7846a4ebe8540ad07331ee1b5"
     ]
    },
    "id": "x1LDNN4CKJNJ",
    "outputId": "a9197162-c6d1-49c5-9741-322234aae8dd"
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, \n",
    "  as_supervised=True, with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv_Oq-qbekcD"
   },
   "source": [
    "Once downloaded, we write a function to preprocess the MNIST dataset ready for use in a neural network. The images come in `uint8` format, and so to normalize the dataset so that all values are between `[0, 1]` we divde by `255` (the max `uint8` value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2WykZXvKMcB"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOHsvHJFfBZK"
   },
   "source": [
    "Let's apply this function to the dataset using `.map` and take a batch size of `128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9ifl7ztLp-K"
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(\n",
    "  normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(\n",
    "  normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0CIYup1fP3j"
   },
   "source": [
    "We are now ready to create the model using the `Sequential` functionality. \n",
    "\n",
    "Although we could achieve a model with high accuracy using a fully connected model, this would require a lot of weights and biases. The Ethos-U55 is designed to be used with a Cortex-M55 meaning there will be memory limits. For this reason we build a convolutional network with large kernel sizes to reduce the number of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXs4ohIBLtJs"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28,28,1)),\n",
    "  tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "  tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC85LSpgcW5"
   },
   "source": [
    "We are now ready to train the model. For this toy example we will just train for a singular epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hQWCshvgh3X",
    "outputId": "81d893d9-a1cf-445d-d86c-062558279b79"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(0.001), \n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(ds_train, epochs=1, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWWHoEwEgrgn"
   },
   "source": [
    "## Quantize the Model\n",
    "\n",
    "The next step is to quantize the model. This converts the weights from floating-point numbers to integer numbers. The Ethos-U55 supports 8 bit weights, and 8 bit and 16 bit activations. \n",
    "\n",
    "In this example we will quantize the model into `int8` format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Itj03AI3g-3u"
   },
   "source": [
    "Let's first `unbatch` the dataset from 128 samples at a time. In inference we will only be running one image at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6mQX5slMVRi"
   },
   "outputs": [],
   "source": [
    "ds_train = ds_train.unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F16ebTLfhIbS"
   },
   "source": [
    "We can then build a generator function to use in the conversion process. \n",
    "\n",
    "Creating a generator allows the TensorFlow Lite converter find the best weights to fall to based on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK0O6BZxhHvW"
   },
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value, output_value in ds_train.batch(1).take(100):\n",
    "    yield [input_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cu6-dIGhUOR"
   },
   "source": [
    "Finally we are ready to convert the model. We can use the `from_keras_model` method to create a converter from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G74vW7fHhWwO"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cseaAGpahiGw"
   },
   "source": [
    "We can then set the `inference_input_type`, `inference_output_type` and `supported_ops` to `int8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfERi-ZihhnQ"
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFkw1yKeh9MR"
   },
   "source": [
    "We then add the `representative_dataset` to be our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51eB1Qcph5fP"
   },
   "outputs": [],
   "source": [
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ9plu3riC8S"
   },
   "source": [
    "The last step is to run the conversion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dJDoeRYiDXW",
    "outputId": "d3f6682c-ea18-4d87-fcba-907b48b91d82"
   },
   "outputs": [],
   "source": [
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G02dEuPeiG87"
   },
   "source": [
    "We now have a quantized model in TFLite format. Let's save this to our files as `my_model.tflite`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0fUWRdtNd2S"
   },
   "outputs": [],
   "source": [
    "with open(\"my_model.tflite\", \"wb\") as f:\n",
    "  f.write(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "freVF5X5iZOm"
   },
   "source": [
    "## Vela Compiler\n",
    "\n",
    "When creating a model for use on Ethos-U55 we need to use the Vela Compiler to optimise the model.\n",
    "\n",
    "This is a command-line tool written in Python which takes a `.tflite` file and outputs another `.tflite` file. The new file is restructured in a way that Ethos-U understands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSWWZxa3jHqN"
   },
   "source": [
    "To do this, let's first install `ethos-u-vela` for the compiler and `xxd` which will be used to convert binary files into hexdumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhxBhPy2Lw_e",
    "outputId": "6d794fa9-bde1-4176-dde5-aa342a04d672"
   },
   "outputs": [],
   "source": [
    "!pip install ethos-u-vela\n",
    "!apt install -y xxd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIYnCP6FjkRL"
   },
   "source": [
    "We can now compile the model. For this we will specify the config as `ethos-u55-128`. This is one of the commonly used templates for Ethos-U55. This configuration has 128 macs. We will create a `vela.ini` file with our system configuration description. This information helps vela to optimize model efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vela.ini\n",
    "\n",
    "[System_Config.Ethos_U55_High_End_Embedded]\n",
    "core_clock=500e6\n",
    "axi0_port=Sram\n",
    "axi1_port=OffChipFlash\n",
    "Sram_clock_scale=1.0\n",
    "Sram_burst_length=32\n",
    "Sram_read_latency=32\n",
    "Sram_write_latency=32\n",
    "OffChipFlash_clock_scale=0.125\n",
    "OffChipFlash_burst_length=128\n",
    "OffChipFlash_read_latency=64\n",
    "OffChipFlash_write_latency=64\n",
    "\n",
    "; Shared SRAM: the SRAM is shared between the Ethos-U and the Cortex-M software\n",
    "; The non-SRAM memory is assumed to be read-only\n",
    "[Memory_Mode.Shared_Sram]\n",
    "const_mem_area=Axi1\n",
    "arena_mem_area=Axi0\n",
    "cache_mem_area=Axi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfaE1o5VTLmm",
    "outputId": "32f7d41c-5bfa-4bef-bff4-3825d1da3ff1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "vela --accelerator-config=ethos-u55-128 \\\n",
    "--optimise Performance \\\n",
    "--memory-mode=Shared_Sram \\\n",
    "--system-config=Ethos_U55_High_End_Embedded \\\n",
    "--config vela.ini \\\n",
    "my_model.tflite "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiNyZX9lj1Oe"
   },
   "source": [
    "We can then convert the `.tflite` binary into a hexdump C headerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f-vZlyhj0rm",
    "outputId": "e5f468d3-012a-4579-8c1e-d92654af5b5a"
   },
   "outputs": [],
   "source": [
    "!xxd -i output/my_model_vela.tflite my_network_model.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HceA9y8Oj_hF"
   },
   "source": [
    "The last step is to do some cleaning up of the file for the application. Here we rename the model from `output_my_model_vela_tflite` to `network_model` and add some header guards to the file.\n",
    "\n",
    "The most important is to add model variable attribute `__attribute__((aligned(16)))` for 16 bytes alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t2Y0Aapj_N3",
    "outputId": "8067d046-abb2-4230-c7b8-13f52322790a"
   },
   "outputs": [],
   "source": [
    "!sed -i 's/unsigned int output_my_model_vela_tflite_len/const unsigned int network_model_len/' my_network_model.h\n",
    "!sed -i 's/unsigned char output_my_model_vela_tflite\\[\\]/const unsigned char network_model\\[\\] __attribute__((aligned(16)))/' my_network_model.h\n",
    "\n",
    "!sed -i '1s/^/#define NETWORK_MODEL_H\\n/' my_network_model.h\n",
    "!sed -i '1s/^/#ifndef NETWORK_MODEL_H\\n/' my_network_model.h\n",
    "!echo \"#endif //NETWORK_MODEL_H\" >> my_network_model.h"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_and_deploy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07a92d5a2ba94fbfa98f8e69e858d1b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20200e22b37c40f38d0cae9d63828d61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a866c812aefb42709c286b504c4b502a",
       "IPY_MODEL_cc2f6b02e2fa4e6b9ba5cadb7c940e38"
      ],
      "layout": "IPY_MODEL_d0332e909a1e41d4b79c78d872426973"
     }
    },
    "2e5a1d3859f74f7ebf8e3a0436276e4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a866c812aefb42709c286b504c4b502a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Dl Completed...: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07a92d5a2ba94fbfa98f8e69e858d1b7",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bedb6318f10e47768eed2b0a76f80169",
      "value": 4
     }
    },
    "b3d302a7846a4ebe8540ad07331ee1b5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bedb6318f10e47768eed2b0a76f80169": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cc2f6b02e2fa4e6b9ba5cadb7c940e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3d302a7846a4ebe8540ad07331ee1b5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2e5a1d3859f74f7ebf8e3a0436276e4f",
      "value": " 4/4 [00:00&lt;00:00,  4.40 file/s]"
     }
    },
    "d0332e909a1e41d4b79c78d872426973": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
